{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3V3tNoJVYDF"
   },
   "source": [
    "# Hands on : introduction to NN on HEP dataset\n",
    "\n",
    "This tutorial will be focused on the classification problem in a physics example: training a neural network classifier to distinguish the Higgs boson signals from background events.\n",
    "\n",
    "In this tutorial, we will use TensorFlow and Keras to train our neural networks (different from the first hands on tutorial which uses PyTorch), but the concept is the same, and we will be focused more on data processing, model optimization and overtraining.\n",
    "\n",
    "Outline:\n",
    "- Load data from root files\n",
    "- Explore the data and weights\n",
    "- Preprocess data for training\n",
    "- Train a neural network\n",
    "- Quantify its performance\n",
    "- Tune the model and overtraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT7-MMpfrlHR"
   },
   "source": [
    "## Intall and import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FO_W4IvbVYDL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np  # numpy is a python package for data processing\n",
    "import pandas as pd  # pandas is a python package for data processing\n",
    "import matplotlib.pyplot as plt  # matplotlib is a powerful package for plotting results\n",
    "import uproot  # uproot is a useful python package to read data from root files\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.metrics import AUC\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # to see all columns of df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHSzcuTgVYDT",
    "tags": []
   },
   "source": [
    "## Load events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data were created from ATLAS Open Data:\n",
    "\n",
    "http://opendata.atlas.cern/release/2020/documentation/datasets/intro.html\n",
    "\n",
    "The dataset we will be using contains MC simulation of the Standard Model Higgs boson production events (signal), as well as Standard Model background, including top-quark-pair production, single-top production, production of weak bosons in association with jets (W+jets, Z+jets) and production of a pair of bosons (diboson WW, WZ, ZZ).\n",
    "\n",
    "All the data are stored in a root file, which contains different kinematic variables of each event, the event label (`label`) to indicate whether an event is a signal or background, as well as the MC event weights (`mcWeight`). We use [uproot](https://indico.cern.ch/event/686641/contributions/2894906/attachments/1606247/2548596/pivarski-uproot.pdf) to read data from the root file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kl-4W4Ifs8EV",
    "outputId": "54d366bc-8925-4c6a-e867-ef70016e016b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"dataWW_d1.root\"\n",
    "file = uproot.open(filename)\n",
    "\n",
    "# show what is inside the root file loaded from uproot\n",
    "print(file.classnames())\n",
    "\n",
    "tree = file[\"tree_event\"]  # select the TTree inside the root file\n",
    "tree.show()  # show all the branches inside the TTree\n",
    "dfall = tree.arrays(library=\"pd\")  # convert uproot TTree into pandas dataframe\n",
    "print(\"============================================\")\n",
    "print(\"File loaded with \", len(dfall), \" events \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xNeCAIuFVYDZ",
    "outputId": "b12cbd30-536e-429c-a1be-677ee0b937fb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dump list of feature\n",
    "dfall.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "j9l7wkkorlHe",
    "outputId": "7d824317-2287-44b0-e153-112ecf7ddc13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# examine first few events\n",
    "dfall.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtI5u5GErlHq"
   },
   "source": [
    "## Event Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we want to focus on events with exactly 2 leptons. Therefore, we will select only events with exactly two leptons:\n",
    "\n",
    "`dfall.lep_n == 2`\n",
    "\n",
    "In addition, we only keep events with positive event weight (the column `mcWeight`) to make it simple. This is in principle wrong, but many Data Science tools break given a negative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaO2JM1hrlHr",
    "outputId": "bc59e98d-f6f1-42b7-e9f3-48052fe4ebc9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of events before selections:\", len(dfall))\n",
    "fulldata = dfall[\n",
    "    ### Complete this line to select only events with exactly two leptons and positive event weights\n",
    "]\n",
    "print(\"Number of events after selections:\", len(fulldata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `label` is whether an event is a signal (Higgs boson events) or a background. An event with the `label == 1` means it is a signal event, otherwise (`label == 0`) it is a background event.\n",
    "\n",
    "The event weights (`mcWeight`) will be used during the training. The event with a larger weight will have a larger contribution in the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMQpKhDKrlH0",
    "outputId": "ccb36177-999c-4203-939e-4e04383c1954",
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = fulldata.label\n",
    "weights = fulldata.mcWeight\n",
    "\n",
    "print(\"Number of selected signal events:\", len(fulldata[target == 1]))\n",
    "print(\"Number of selected background events:\", len(fulldata[target == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For simplicity, we'll only use some features on the first pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "6e0Hlpv6rlH4",
    "outputId": "8705b9a0-9dcc-4eb6-ab49-b3fd0b8ea1a6",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(fulldata, columns=[\"met_et\", \"met_phi\", \"lep_pt_0\", \"lep_pt_1\", 'lep_phi_0', 'lep_phi_1'])\n",
    "\n",
    "# Or we can use more features to improve the discrimination power:\n",
    "# data=pd.DataFrame(fulldata, columns=[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_eta_0', 'lep_eta_1',\n",
    "#                                      'lep_phi_0', 'lep_phi_1','jet_n','jet_pt_0',\n",
    "#                                      'jet_pt_1', 'jet_eta_0', 'jet_eta_1', 'jet_phi_0', 'jet_phi_1'])\n",
    "\n",
    "# We can also engineer our own feature:\n",
    "# data[\"lep_deltaphi\"]=np.abs(np.mod(data.lep_phi_1-data.lep_phi_0+3*np.pi,2*np.pi)-np.pi)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "o2mf1bLVrlH7",
    "outputId": "b4484ca0-0983-4477-9af7-26bf496aa618",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = data[target == 0].hist(\n",
    "    weights=weights[target == 0], figsize=(15, 12), color='b', alpha=0.4, density=True, label=\"signal\"\n",
    ")\n",
    "ax = ax.flatten()[: data.shape[1]]  # to avoid error if holes in the grid of plots (like if 7 or 8 features)\n",
    "data[target == 1].hist(\n",
    "    weights=weights[target == 1], figsize=(15, 12), color='r', alpha=0.4, density=True, ax=ax, label=\"background\"\n",
    ")\n",
    "for i in range(len(ax)):\n",
    "    ax[i].legend(loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kowHjX4rlIC"
   },
   "source": [
    "## Data splitting\n",
    "\n",
    "It is very common in machine learning to split data into multiple independent sets, and only use part of the data for training/optimizing the machine learning models, and the rest for testing/evaluating performance.\n",
    "\n",
    "In the following, we will split the whole data into 50% training set, 25% validation set and 25% test set:\n",
    "\n",
    "- __Training Dataset:__ The sample of data used to fit the model.\n",
    "- __Validation Dataset:__ The sample used to provide an unbiased evaluation of a model fit on the training dataset while tuning  hyperparameters.\n",
    "- __Test Dataset:__ The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "\n",
    "The API referecne of the `train_test_split` function:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data into training set and rest (validation + test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9j5hdrmrlID",
    "outputId": "75c0925f-74c9-4e24-d169-8a884b510f4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(\n",
    "    X_train,\n",
    "    X_val_and_test,\n",
    "    y_train,\n",
    "    y_val_and_test,\n",
    "    weights_train,\n",
    "    weights_val_and_test\n",
    ") = train_test_split(data, target, weights, train_size=0.5)\n",
    "\n",
    "print(\"X_train Shape: \", X_train.shape)\n",
    "print(\"y_train Shape: \", y_train.shape)\n",
    "print(\"Training Weights shape: \", weights_train.shape, \"\\n\")\n",
    "print(\"X_val_and_test Shape: \", X_val_and_test.shape)\n",
    "print(\"y_val_and_test Shape: \", y_val_and_test.shape)\n",
    "print(\"weights_val_and_test shape: \", weights_val_and_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the rest (val + test) into validation set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    X_test,\n",
    "    X_val,\n",
    "    y_test,\n",
    "    y_val,\n",
    "    weights_test,\n",
    "    weights_val,\n",
    ") = ### Copmlete this line to further split to validation and test set\n",
    "\n",
    "print(\"X_train Shape: \", X_train.shape)\n",
    "print(\"y_train Shape: \", y_train.shape)\n",
    "print(\"weights_train shape: \", weights_train.shape, \"\\n\")\n",
    "print(\"X_val Shape: \", X_val.shape)\n",
    "print(\"y_val Shape: \", y_val.shape)\n",
    "print(\"weights_val shape: \", weights_val.shape, \"\\n\")\n",
    "print(\"X_test Shape: \", X_test.shape)\n",
    "print(\"y_test Shape: \", y_test.shape)\n",
    "print(\"weights_test shape: \", weights_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Data\n",
    "\n",
    "When optimizing the node weights of a neural netowrk, the input scale can significantly affect the performance. That is, unscaled (raw) input variables can result in a slow or unstable learning process. We would therefore like to rescale input variables prior to training a neural network model.\n",
    "\n",
    "There are 2 common ways to rescale the data:\n",
    "\n",
    "1. **Scale to Mean of 0 and Variance of 1.0:**   $\\ \\ \\ \\ x^\\prime = (x-\\mu)/\\sigma$\n",
    "2. **Scale to Max of 1 and Min of 0:**   $\\ \\ \\ \\ x^\\prime = (x-x_{\\mathrm{min}})/(x_{\\mathrm{max}}-x_{\\mathrm{min}})$\n",
    "\n",
    "In the following, we will go with the first approach, but feel free to try the second approach! (hint: use [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Before standardization:\")\n",
    "print(\"X_train mean:\", X_train.to_numpy().mean(axis=0).round(2), \"std:\",  X_train.to_numpy().std(axis=0).round(2))\n",
    "print(\"X_val mean:\", X_val.to_numpy().mean(axis=0).round(2), \"std:\",  X_val.to_numpy().std(axis=0).round(2))\n",
    "print(\"X_test mean:\", X_test.to_numpy().mean(axis=0).round(2), \"std:\",  X_test.to_numpy().std(axis=0).round(2), \"\\n\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"After standardization:\")\n",
    "print(\"X_train mean, std: \", X_train.mean(axis=0).round(2), X_train.std(axis=0).round(2))\n",
    "print(\"X_val mean, std: \", X_val.mean(axis=0).round(2), X_val.std(axis=0).round(2))\n",
    "print(\"X_test mean, std: \", X_test.mean(axis=0).round(2), X_test.std(axis=0).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust the Signal/Background Weights for balanced training\n",
    "Naturally, we would like to have an equal emphasis on the signal and background during the training. To do so, we can \"normalize\" the signal weights, and background weights such that the sum of weights is the same between signal and background. Note that the adjustment is only needed for train and validation sets, as we do not use test set to calculate the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sum of training signal weights before normalization:\", weights_train[y_train == 1].sum())\n",
    "print(\"Sum of training background weights before normalization:\", weights_train[y_train == 0].sum())\n",
    "print(\"Sum of validation signal weights before normalization:\", weights_val[y_val == 1].sum())\n",
    "print(\"Sum of validation background weights before normalization:\", weights_val[y_val == 0].sum(), \"\\n\")\n",
    "\n",
    "weights_train[y_train == 0] *= len(weights_train) / weights_train[y_train == 0].sum()\n",
    "weights_train[y_train == 1] *= len(weights_train) / weights_train[y_train == 1].sum()\n",
    "\n",
    "weights_val[y_val == 0] *= len(weights_val) / weights_val[y_val == 0].sum()\n",
    "weights_val[y_val == 1] *= len(weights_val) / weights_val[y_val == 1].sum()\n",
    "\n",
    "print(\"Sum of training signal weights after normalization:\", weights_train[y_train == 1].sum())\n",
    "print(\"Sum of training background weights after normalization:\", weights_train[y_train == 0].sum())\n",
    "print(\"Sum of validation signal weights after normalization:\", weights_val[y_val == 1].sum())\n",
    "print(\"Sum of validation background weights after normalization:\", weights_val[y_val == 0].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxybCOi-rlIM"
   },
   "source": [
    "## Training the neural network\n",
    "\n",
    "In the following, we will use TensorFlow and Keras to build our neural network and train the model.\n",
    "\n",
    "The architecture is pretty simple: A fully-connected network with 2 hidden layers with the ReLU activation function.\n",
    "The last layer uses the Sigmoid activation to output a classifier score that ranges from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.Input((X_train.shape[1],)), # input layer\n",
    "        tf.keras.layers.Dense(128, activation='relu'),  # layer 1\n",
    "        ### Complete this line to add another layer with 128 nodes and ReLU activation function, and the last layer with sigmoid activation\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the binary cross-entropy as the loss function, and use the Adam optimizer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",  # use loss=\"mean_squared_error\" for MSE loss\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),  # tf.keras.optimizers.experimental.SGD for the SGD optimizer\n",
    "    weighted_metrics=[AUC(name=\"auc\")],  # monitor AUC for each epoch during the training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we train the model with 10 epochs with a batch size of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Start training!\")\n",
    "\n",
    "# model.fit() performs the training\n",
    "\n",
    "the_fit = model.fit(\n",
    "    X_train,\n",
    "    y_train.values,\n",
    "    sample_weight=weights_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val, y_val, weights_val),\n",
    "    # callbacks=[K.callbacks.EarlyStopping(monitor='val_auc', patience=10, restore_best_weights=True, mode='max')], # uncomment to use early stopping (should also increase the epochs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training progress\n",
    "\n",
    "Below, we plot the training loss, validation loss, training AUC and validation AUC per epoch.\n",
    "\n",
    "Ideally, we should see the loss going down, and AUC going up with the number of epochs trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(the_fit.history['loss'], color=\"tab:blue\")\n",
    "ax.plot(the_fit.history['val_loss'], color=\"tab:blue\", ls=\"--\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=14)\n",
    "ax.set_ylabel(\"Loss\", fontsize=14)\n",
    "ax.tick_params(width=2, grid_alpha=0.5, labelsize=12)\n",
    "ax.grid(True, axis='y')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel(\"ROC AUC\", color='tab:orange', fontsize=14)\n",
    "ax2.tick_params(width=2, grid_alpha=0.5, labelsize=12, axis='y', labelcolor='tab:orange')\n",
    "ax2.plot(the_fit.history['auc'], color='tab:orange', lw=2)\n",
    "ax2.plot(the_fit.history['val_auc'], color='tab:orange', ls='--', lw=2)\n",
    "# ax2.set_ylim([0, 1])\n",
    "\n",
    "ax2.plot([], [], color=\"black\", label=\"training set\")\n",
    "ax2.plot([], [], color=\"black\", ls='--', label=\"validation set\")\n",
    "ax2.legend(fontsize=14, frameon=False, loc=\"right\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to make predicions!\n",
    "Evaluate the model based on predictions made with X_test $\\rightarrow$ y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test).flatten()\n",
    "y_pred_train = model.predict(X_train).flatten()\n",
    "\n",
    "print(\"y_pred_test: \", y_pred_test)\n",
    "print(\"y_pred_train: \", y_pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves and Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr_test, tpr_test, _ = roc_curve(y_true=y_test, y_score=y_pred_test, sample_weight=weights_test)\n",
    "fpr_train, tpr_train, _ = roc_curve(y_true=y_train, y_score=y_pred_train, sample_weight=weights_train)\n",
    "auc_test = roc_auc_score(y_true=y_test, y_score=y_pred_test, sample_weight=weights_test)\n",
    "auc_train = roc_auc_score(y_true=y_train.values, y_score=y_pred_train, sample_weight=weights_train)\n",
    "plt.plot(fpr_test, tpr_test, color='tab:blue', lw=2, ls=\"--\", label=f\"Test set auc: {auc_test:.4f}\")\n",
    "plt.plot(fpr_train, tpr_train, color='tab:orange', lw=2, ls=\":\", label=f\"Training set auc: {auc_train:.4f}\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting NN Score for Signal and Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from extra_functions import compare_train_test\n",
    "\n",
    "compare_train_test(\n",
    "    y_pred_train,\n",
    "    y_train,\n",
    "    y_pred_test,\n",
    "    y_test,\n",
    "    xlabel=\"NN Score\",\n",
    "    title=\"NN\",\n",
    "    weights_train=weights_train.values,\n",
    "    weights_test=weights_test.values,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIsMSGl-Kwql",
    "tags": []
   },
   "source": [
    "## Significance Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a standard statistical analysis for a search, an observed significance is obtained. This corresponds to the significance with which we reject the background-only hypothesis. That is, the higher the significance, the more likely the signal we are searching for exists.\n",
    "\n",
    "We can use the formula below to obtain an estimated value of the expected significance.\n",
    "\n",
    "$Z = \\sqrt{2+((s+b)\\ln(1+s/b)-s)}$\n",
    "\n",
    "**see [arXiv:1007.1727](https://arxiv.org/pdf/1007.1727.pdf) [Eq. 97]**\n",
    "\n",
    "This corresponds to the signal sensitivity. This is a good indication of the performance of our classification model. => We want the significance to be as high as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qubY3CMNKwql",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from math import log\n",
    "\n",
    "\n",
    "def amsasimov(s, b):\n",
    "    if b <= 0 or s <= 0:\n",
    "        return 0\n",
    "    try:\n",
    "        return sqrt(2 * ((s + b) * log(1 + float(s) / b) - s))\n",
    "    except ValueError:\n",
    "        print(1 + float(s) / b)\n",
    "        print(2 * ((s + b) * log(1 + float(s) / b) - s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use the classifier score to separate signal and background is to make a cut on the classifier score for all the events. In the following, we scan through the cut value and look at the corresponding significance. This can help us determine the optimal cut value of the classifier score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "int_pred_test_sig = [weights_test[(y_test == 1) & (y_pred_test > th_cut)].sum() for th_cut in np.linspace(0, 1, num=50)]\n",
    "int_pred_test_bkg = [weights_test[(y_test == 0) & (y_pred_test > th_cut)].sum() for th_cut in np.linspace(0, 1, num=50)]\n",
    "vamsasimov = [amsasimov(sumsig, sumbkg) for (sumsig, sumbkg) in zip(int_pred_test_sig, int_pred_test_bkg)]\n",
    "Z = max(vamsasimov)\n",
    "\n",
    "plt.plot(np.linspace(0, 1, num=50), vamsasimov, label=f'Significance ($Z_{{max}} = {np.round(Z, decimals=2)}$)')\n",
    "plt.title(\"NN Significance\")\n",
    "plt.xlabel(\"Cut Value\")\n",
    "plt.ylabel(\"Significance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does overtraining look like?\n",
    "\n",
    "Recipe:\n",
    "1. Add More layers\n",
    "2. Add more nodes per layer\n",
    "3. Train on less data\n",
    "4. Train for more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Crazy Example\n",
    "N = len(X_train)\n",
    "n = int(N / 10000)\n",
    "print(\"Using\", n, \"/\", N, \"events\")\n",
    "\n",
    "X_small = X_train[:n]\n",
    "y_small = y_train[:n]\n",
    "weights_small = weights_train[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ot_model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.Input((X_small.shape[1],)),  # input layer\n",
    "        tf.keras.layers.Dense(512, activation='relu'),  # 1st hiddden layer\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "ot_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", weighted_metrics=[AUC(name=\"auc\")])\n",
    "\n",
    "the_overfit = ot_model.fit(\n",
    "    X_small,\n",
    "    y_small.values,\n",
    "    sample_weight=weights_small,\n",
    "    epochs=25,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val, y_val, weights_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(the_overfit.history['loss'], color=\"tab:blue\")\n",
    "ax.plot(the_overfit.history['val_loss'], color=\"tab:blue\", ls=\"--\")\n",
    "ax.set_xlabel(\"Epoch\", fontsize=14)\n",
    "ax.set_ylabel(\"Loss\", fontsize=14)\n",
    "ax.tick_params(width=2, grid_alpha=0.5, labelsize=12)\n",
    "ax.grid(True, axis='y')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel(\"ROC AUC\", color='tab:orange', fontsize=14)\n",
    "ax2.tick_params(width=2, grid_alpha=0.5, labelsize=12, axis='y', labelcolor='tab:orange')\n",
    "ax2.plot(the_overfit.history['auc'], color='tab:orange', lw=2)\n",
    "ax2.plot(the_overfit.history['val_auc'], color='tab:orange', ls='--', lw=2)\n",
    "# ax2.set_ylim([0, 1])\n",
    "\n",
    "ax2.plot([], [], color=\"black\", label=\"training set\")\n",
    "ax2.plot([], [], color=\"black\", ls='--', label=\"validation set\")\n",
    "ax2.legend(fontsize=14, frameon=False, loc=\"right\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ot_y_pred_test = ot_model.predict(X_test).flatten()\n",
    "ot_y_pred_train = ot_model.predict(X_small).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr_test, tpr_test, _ = roc_curve(y_true=y_test, y_score=ot_y_pred_test, sample_weight=weights_test)\n",
    "fpr_train, tpr_train, _ = roc_curve(y_true=y_small, y_score=ot_y_pred_train, sample_weight=weights_small)\n",
    "auc_test = roc_auc_score(y_true=y_test, y_score=ot_y_pred_test, sample_weight=weights_test)\n",
    "auc_train = roc_auc_score(y_true=y_small.values, y_score=ot_y_pred_train, sample_weight=weights_small)\n",
    "plt.plot(fpr_test, tpr_test, color='tab:blue', lw=2, ls=\"--\", label=f\"Test set auc: {auc_test:.4f}\")\n",
    "plt.plot(fpr_train, tpr_train, color='tab:orange', lw=2, ls=\":\", label=f\"Training set auc: {auc_train:.4f}\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JKKJbnWX7Ql"
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQJuC2-1UkqP"
   },
   "source": [
    "### Can we try to improve the neural network performance?\n",
    "\n",
    "Hints:\n",
    "- Adjust learning rate\n",
    "- Adjust the number of neurons, and layers\n",
    "- Increase epochs\n",
    "- Enable early stopping (and increase max epochs)\n",
    "- Adjust batch size\n",
    "- Change activations: [relu, leakyrelu, selu, tanh]\n",
    "- Try different optimizers (e.g. SGD)\n",
    "- Dynamic learning rate: e.g. tfa.optimizers.RectifiedAdam as the optimizer, tf.keras.callbacks.ReduceLROnPlateau as a callback\n",
    "- Use a more complicated structure? e.g. RNN, deep sets, GNN, etc\n",
    "- More... google!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when we add more features or engineer aditional Features?\n",
    "\n",
    "Try it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when we don't standardize our inputs?\n",
    "\n",
    "Try it yourself!\n",
    "\n",
    "What about a different prescaling (using [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) for example?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when we don't use balanced weights for signal and background?\n",
    "\n",
    "Use the raw MC weights and retrain the model.\n",
    "\n",
    "Does the \"natural\" weight always give the best performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HEPML_HandsOn_NN_veryold.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
